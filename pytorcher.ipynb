{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edaed094-3599-48d0-8870-bd7d906dad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c914c1d1-875f-463b-80c1-e4b50bb04a69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  4 00:24:50 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:03:00.0  On |                  N/A |\n",
      "|  0%   57C    P2    58W / 170W |  11977MiB / 12288MiB |     63%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3694      G   /usr/lib/xorg/Xorg                270MiB |\n",
      "|    0   N/A  N/A      4519      G   /usr/bin/gnome-shell               37MiB |\n",
      "|    0   N/A  N/A      5570      G   ...RendererForSitePerProcess       24MiB |\n",
      "|    0   N/A  N/A      5607      G   ...Process --enable-crashpad       31MiB |\n",
      "|    0   N/A  N/A     19745      G   ...in,WebAssemblyTrapHandler       93MiB |\n",
      "|    0   N/A  N/A    154365      G   /usr/bin/nautilus                  22MiB |\n",
      "|    0   N/A  N/A    156712      G   ...RendererForSitePerProcess       84MiB |\n",
      "|    0   N/A  N/A    276349      G   ...e/Steam/ubuntu12_32/steam        2MiB |\n",
      "|    0   N/A  N/A    276550      G   ...buntu12_64/steamwebhelper        3MiB |\n",
      "|    0   N/A  N/A    339544      C   .../AI/first/venv/bin/python      540MiB |\n",
      "|    0   N/A  N/A    340104      C   python3.10                      10858MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e2749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.set_default_device(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('clear')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53b71f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12629377024, 37748736, 34330624, 3418112)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "t, r, a, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a484a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 8196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "029d47fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacty of 11.76 GiB of which 66.31 MiB is free. Including non-PyTorch memory, this process has 540.00 MiB memory in use. Process 340104 has 10.60 GiB memory in use. Of the allocated memory 32.74 MiB is allocated by PyTorch, and 3.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mSIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m X2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m,SIZE,SIZE)\n\u001b[1;32m      5\u001b[0m X\u001b[38;5;241m.\u001b[39mdevice, X2\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/data/Projects/AI/first/venv/lib/python3.11/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacty of 11.76 GiB of which 66.31 MiB is free. Including non-PyTorch memory, this process has 540.00 MiB memory in use. Process 340104 has 10.60 GiB memory in use. Of the allocated memory 32.74 MiB is allocated by PyTorch, and 3.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "X = torch.rand(3,SIZE,SIZE)\n",
    "X2 = torch.rand(3,SIZE,SIZE)\n",
    "\n",
    "X.device, X2.device\n",
    "# torch.matmul(X, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "t, r, a, f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
